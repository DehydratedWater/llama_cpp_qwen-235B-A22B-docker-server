# Qwen3-235B Docker Configuration
# Copy this file to .env and adjust the values for your setup

# Model path - adjust to your actual model location
MODEL_PATH=/mnt/ssd-4tb/ai_models/text-generation-webui/user_data/models/Qwen3-235B-A22B-Instruct-2507-GGUF/Q2_K_L/Qwen3-235B-A22B-Instruct-2507-Q2_K_L-00001-of-00002.gguf

# GPU configuration - adjust based on your hardware
# Format: gpu1_memory,gpu2_memory,gpu3_memory,gpu4_memory (in GB)
# Example: 24,24,24,16.8 for 3x24GB GPUs + 1x16GB A4000 at 0.7 capacity
TENSOR_SPLIT=24,24,24,16.8

# Context size (80k = 81920 tokens)
CONTEXT_SIZE=81920

# Server settings
HOST=0.0.0.0
PORT=8080

# Performance settings
THREADS=16
PARALLEL_REQUESTS=4
BATCH_SIZE=2048

# Flash attention (true/false)
FLASH_ATTENTION=true